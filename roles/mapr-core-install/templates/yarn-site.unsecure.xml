<configuration>
    <!-- Resource Manager MapR HA Configs -->
    <property>
        <name>yarn.resourcemanager.ha.custom-ha-enabled</name>
        <value>true</value>
        <description>MapR Zookeeper based RM Reconnect Enabled. If this is true, set the failover proxy to be the class
            MapRZKBasedRMFailoverProxyProvider
        </description>
    </property>
    <property>
        <name>yarn.client.failover-proxy-provider</name>
        <value>org.apache.hadoop.yarn.client.MapRZKBasedRMFailoverProxyProvider</value>
        <description>Zookeeper based reconnect proxy provider. Should be set if and only if mapr-ha-enabled property is
            true.
        </description>
    </property>
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
        <description>RM Recovery Enabled</description>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.custom-ha-rmaddressfinder</name>
        <value>org.apache.hadoop.yarn.client.MapRZKBasedRMAddressFinder</value>
    </property>
    {% if mapr_security != 'none' %}
    <property>
        <name>yarn.acl.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.admin.acl</name>
        <value>{{ mapr_user }}</value>
    </property>
    {% endif %}

    <!-- :::CAUTION::: DO NOT EDIT ANYTHING ON OR ABOVE THIS LINE -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>{{ mapr_group }}</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>{{ "true" if yarn_log_aggregation else "false" }}</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>{{ yarn_max_container_allocation_in_mb }}</value>
        <description>The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this will throw a InvalidResourceRequestException.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.am.max-attempts</name>
        <value>4</value>
        <description>The maximum number of application attempts</description>
    </property>
    {% if yarn_local_dir_to_maprfs %}
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/mapr/{{ cluster_name }}/var/mapr/local/{{ ansible_fqdn }}/nm-local-dir</value>
    </property>
    {% endif %}
    {% if yarn_spark_shuffle %}
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle,mapr_direct_shuffle,spark_shuffle</value>
        <description>shuffle service that needs to be set for Map Reduce to run</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
        <value>org.apache.spark.network.yarn.YarnShuffleService</value>
    </property>
    <property>
        <name>spark.shuffle.service.port</name>
        <value>7337</value>
        <description>Port on which the external shuffle service will run.</description>
    </property>
    {% if spark_sasl_encryption %}
    <property>
        <name>spark.authenticate</name>
        <value>true</value>
        <description>Whether Spark authenticates its internal connections.</description>
    </property>
    {% endif %}
    <property>
        <name>spark.yarn.shuffle.stopOnFailure</name>
        <value>false</value>
        <description>Whether to stop the NodeManager when there's a failure in the Spark Shuffle Service's initialization.
            This prevents application failures caused by running containers on NodeManagers where the Spark Shuffle Service is not running.
        </description>
    </property>
    {% endif %}
</configuration>
